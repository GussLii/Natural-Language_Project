{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch.nn as nn\n",
    "from src.load_dataloader import initial_dataloader_vector_slicing\n",
    "from src.evaluation import initial_LSTM\n",
    "from src.load_config import load_config\n",
    "from src.evaluation import show_sentence\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 8\n",
    "max_length = 128\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, device = load_config(max_length)\n",
    "# import data\n",
    "df = pd.read_csv('data_preprocess/datasets_combine.csv')\n",
    "train_df, val_df,test_df, train_dataset,val_dataset, test_dataset,train_loader, val_loader, test_loader = initial_dataloader_vector_slicing(df, tokenizer, max_length,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = initial_LSTM(tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(predicted_token_ids, lstm_model):\n",
    "    # convert predicted_token_ids to text first\n",
    "    texts1 = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
    "    # convert back to token ids\n",
    "    predicted_token_ids = tokenizer(texts1, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')['input_ids'].to(device)\n",
    "    \n",
    "    #predicted_token_ids = tokenizer(texts1, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')['input_ids'].to(device)\n",
    "\n",
    "    # Initialize a tensor to store LSTM outputs\n",
    "    lstm_outputs = torch.empty(size=(predicted_token_ids.shape[0], 2)).to(device)  # 2 for binary classification\n",
    "\n",
    "    # Process each item in the batch\n",
    "    for idx, token_ids in enumerate(predicted_token_ids):\n",
    "        lstm_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Assuming lstm_model.predict returns a tensor of shape [1, 2] (probability for each class)\n",
    "            lstm_output = lstm_model(token_ids.flatten())  # Modify this call according to your LSTM model's interface\n",
    "            lstm_outputs[idx] = lstm_output\n",
    "\n",
    "    return lstm_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]c:\\Users\\Ene\\Desktop\\CS505_github\\Natural-Language_Project\\src\\load_dataloader.py:179: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids1': torch.tensor(item['input_ids1'], dtype=torch.long).squeeze(0),\n",
      "c:\\Users\\Ene\\Desktop\\CS505_github\\Natural-Language_Project\\src\\load_dataloader.py:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids2': torch.tensor(item['input_ids2'], dtype=torch.long).squeeze(0),\n",
      "c:\\Users\\Ene\\Desktop\\CS505_github\\Natural-Language_Project\\src\\load_dataloader.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask1': torch.tensor(item['attention_mask1'], dtype=torch.long).squeeze(0),\n",
      "c:\\Users\\Ene\\Desktop\\CS505_github\\Natural-Language_Project\\src\\load_dataloader.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask2': torch.tensor(item['attention_mask2'], dtype=torch.long).squeeze(0),\n",
      "c:\\Users\\Ene\\Desktop\\CS505_github\\Natural-Language_Project\\src\\load_dataloader.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels1': torch.tensor(item['labels1'], dtype=torch.long).squeeze(0),\n",
      "c:\\Users\\Ene\\Desktop\\CS505_github\\Natural-Language_Project\\src\\load_dataloader.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels2': torch.tensor(item['labels2'], dtype=torch.long).squeeze(0),\n",
      " 10%|█         | 1/10 [10:45<1:36:52, 645.88s/it]"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 10\n",
    "model.to(device)\n",
    "loss_classification = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids1 = batch['input_ids1'].to(device)\n",
    "        attention_mask1 = batch['attention_mask1'].to(device)\n",
    "        labels1 = batch['labels1'].to(device)\n",
    "        \n",
    "        input_ids2 = batch['input_ids2'].to(device)\n",
    "        attention_mask2 = batch['attention_mask2'].to(device)\n",
    "        labels2 = batch['labels2'].to(device)\n",
    "        \n",
    "        # first reconstruct the two sentences\n",
    "        \n",
    "        outputs1 = model(input_ids=input_ids1, attention_mask=attention_mask1, labels=labels1)\n",
    "        outputs2 = model(input_ids=input_ids2, attention_mask=attention_mask2, labels=labels2)\n",
    "        \n",
    "        loss = outputs1.loss + outputs2.loss\n",
    "        \n",
    "        # ----------------------- perform style transfer -----------------------\n",
    "        # call model encoder\n",
    "        encoder_outputs1 = model.encoder(input_ids=input_ids1)\n",
    "        latent_vector1 = encoder_outputs1.last_hidden_state\n",
    "        \n",
    "        encoder_outputs2 = model.encoder(input_ids=input_ids2)\n",
    "        latent_vector2 = encoder_outputs2.last_hidden_state\n",
    "\n",
    "        # modify latent vector\n",
    "        latent_vector1_content = latent_vector1[:, :, :384]\n",
    "        latent_vector1_style = latent_vector1[:, :, 384:]\n",
    "        \n",
    "        latent_vector2_content = latent_vector2[:, :, :384]\n",
    "        latent_vector2_style = latent_vector2[:, :, 384:]\n",
    "        \n",
    "        # swap style\n",
    "        modify_latent_vector1 = torch.cat([latent_vector1_content, latent_vector2_style], dim=-1)\n",
    "        modify_latent_vector2 = torch.cat([latent_vector2_content, latent_vector1_style], dim=-1)\n",
    "        \n",
    "        encoder_outputs1.last_hidden_state = modify_latent_vector1\n",
    "        encoder_outputs2.last_hidden_state = modify_latent_vector2\n",
    "        \n",
    "        outputs1 = model(decoder_input_ids = input_ids1, encoder_outputs=encoder_outputs1)\n",
    "        outputs2 = model(decoder_input_ids = input_ids2, encoder_outputs=encoder_outputs2)\n",
    "\n",
    "        logits1 = outputs1.logits\n",
    "        predicted_token_ids1 = torch.argmax(logits1, dim=-1).to(device)\n",
    "        transfer_labels1 = batch['sentence2_style'].to(device)\n",
    "        \n",
    "        logits2 = outputs2.logits\n",
    "        predicted_token_ids2 = torch.argmax(logits2, dim=-1).to(device)\n",
    "        transfer_labels2 = batch['sentence1_style'].to(device)\n",
    "        \n",
    "        # sent to LSTM model\n",
    "        lstm_outputs1 = classify_sentence(predicted_token_ids1,LSTM_model)\n",
    "        lstm_outputs2 = classify_sentence(predicted_token_ids2, LSTM_model)  \n",
    "        \n",
    "        # classification loss\n",
    "        style_loss1 = loss_classification(lstm_outputs1,transfer_labels1)\n",
    "        style_loss2 = loss_classification(lstm_outputs2,transfer_labels2)\n",
    "        loss += style_loss1 + style_loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text1:  I'll not call you tyrant;But this most cruel usage of your queen,Not able to produce more accusationThan your own weak-hinged fancy, something savoursOf tyranny and will ignoble make you,Yea, scandalous to the world\n",
      "transfer text1:  I not call you ty this most  usage of your queen, able to produce more accusorThan your own -hinged fancy, something tavoursOf tyranny and will ignoble make you,Jea  scandalous to the world.\n",
      "original text2:  Investigation focused on pilots reactions to instrument failure, autopilot switching off\n",
      "transfer text2:  focused on ons reactions to instrument failure, autopilot switching off.Wssss\n",
      "--------------------------------------------------\n",
      "original text1:  Jordan Silverstone bought the suit for just £125 from charity shop\n",
      "transfer text1:  Jordanstone from for for from just $225 from the shopssscomings offer just\n",
      "original text2:  First Herald:Harry of Hereford, Lancaster and Derby,Stands here for God, his sovereign and himself,On pain to be found false and recreant,To prove the Duke of Norfolk, Thomas Mowbray,A traitor to his God, his king and him;And dares him to set forward to the fight\n",
      "transfer text2:  , Hereford, Thomas others,Ons here for God, his, and himselfAnd pain to be found false and recreant,To prove the Duke of England, Thomaswbray,A traitor to his God, his king and him;And dares him to set forward to the fights,\n",
      "--------------------------------------------------\n",
      "original text1:  He turned Air Asia into the region's biggest budget airline\n",
      "transfer text1:  ’ out into the’’s budget airlinea\n",
      "original text2:  What canst thou swear by now?KING RICHARD III:The time to come\n",
      "transfer text2:  ????...?? by??:ICHARD:: time to come?\n",
      "--------------------------------------------------\n",
      "original text1:  Sharon, a decorated war vet before being elected, has been in a coma since 2006\n",
      "transfer text1:  ... vet before being elected has  in a since.a 2006\n",
      "original text2:  CORIOLANUS:Know, good mother,I had rather be their servant in my way,Than sway with them in theirs\n",
      "transfer text2:  USUSUSUS:Know, good mother,I had     in my way,...han sway them their:Y\n",
      "--------------------------------------------------\n",
      "original text1:  George H\n",
      "transfer text1:  sssssWWssWsss\n",
      "original text2:  KING HENRY VI:Stay, gentle Margaret, and hear me speak\n",
      "transfer text2:  KINGRY VI S... English and me speak\n",
      "--------------------------------------------------\n",
      "original text1:  Clinton says U\n",
      "transfer text1:  Asssfass W TresssW EnglishssssWes\n",
      "original text2:  POLIXENES:By my white beard,You offer him, if this be so, a wrongSomething unfilial: reason my sonShould choose himself a wife, but as good reasonThe father, all whose joy is nothing elseBut fair posterity, should hold some counselIn such a business\n",
      "transfer text2:  PXXS:Sy white beard,You, it, if this be so, a wrongSomething uniial: mySould choose himself a wife, but as good reasonThe father, all whose joy is nothing elseBut fair posterity, should hold some counselIn such a business\n",
      "--------------------------------------------------\n",
      "original text1:  6million in community marital assets\n",
      "transfer text1:  in communitytal assetssssW Webs Wsss Ws assetsss\n",
      "original text2:  Roman:I am a Roman; and my services are,as you are, against 'em: know you me yet?Volsce:Nicanor? no\n",
      "transfer text2:  ; my... my...? my my services are, againsts you are, against'm: know you me...?Nosce:Nicanor? no my my my\n",
      "--------------------------------------------------\n",
      "original text1:  Alma SweatX only non-surgical treatment to successfully tackle problem\n",
      "transfer text1:  ’-invasive to successfully tackle\n",
      "original text2:  Come, bring away thy pack afterme\n",
      "transfer text2:  Come!!y!!t Ws W\n",
      "--------------------------------------------------\n",
      "original text1:  NEW: Authorities were monitoring Abdul-Latif and co-conspirator\n",
      "transfer text1:  are  the-tif and co-conspiratorssDiv English\n",
      "original text2:  BAPTISTA:I follow you\n",
      "transfer text2:  ISTAI youss :\n",
      "--------------------------------------------------\n",
      "original text1:  Report said risks include CO2 leaks, transport issues and seismic tremors\n",
      "transfer text1:  2 leaks, transport issues and seismic.tremors.\n",
      "original text2:  QUEEN MARGARET:Poor painted queen, vain flourish of my fortune!Why strew'st thou sugar on that bottled spider,Whose deadly web ensnareth thee about?Fool, fool! thou whet'st a knife to kill thyself\n",
      "transfer text2:  GAAN oor painted:, vain  of my fortune!Why strew's  thou  on that bottled spider,Whose deadly web ensnareth thee about?Fooo, fool! t  whet's  a  to kill thyself\n",
      "--------------------------------------------------\n",
      "original text1:  Keys received a message from Smith reading: \"Hey is your mom's name Jackie?\"As an online news producer, Keys knows the power of social media\n",
      "transfer text1:  s a from: \"Hey is your mom's name Jackie?\"A  news producer Keys knows the power of social media\n",
      "original text2:  GLOUCESTER:He that bereft thee, lady, of thy husband,Did it to help thee to a better husband\n",
      "transfer text2:  He bereft thee,,  y husband,Did it to  the  to    husband\n",
      "--------------------------------------------------\n",
      "original text1:  They reported that the men were taken to the mountains while the women and children remained under armed guard in the village\n",
      "transfer text1:  the women  to the mountains while the women and children wereremained under.. ins the\n",
      "original text2:  ROMEO:O single-soled jest, solely singular for thesingleness\n",
      "transfer text2:  ’-­dst, solely for for thes.leness.s\n",
      "--------------------------------------------------\n",
      "original text1:  Explosion happened at about 11:30 a\n",
      "transfer text1:  at happened at about 11:30....ssstW\n",
      "original text2:  MIRANDA:Sir, are not you my father?PROSPERO:Thy mother was a piece of virtue, andShe said thou wast my daughter; and thy fatherWas Duke of Milan; and thou his only heirAnd princess no worse issued\n",
      "transfer text2:  :ar are not you  father?SPERO::y mother was a piece of virtue, andShe said thou hert my daughter; and thy fatherWas of Milan; and thou his only heirAnd princess no worse\n",
      "--------------------------------------------------\n",
      "original text1:  BUCKINGHAM:Fie, what an indirect and peevish courseIs this of hers! Lord cardinal, will your gracePersuade the queen to send the Duke of YorkUnto his princely brother presently?If she deny, Lord Hastings, go with him,And from her jealous arms pluck him perforce\n",
      "transfer text1:  .HAMB.Fie, what an indirect and peevish courseIs: of hers! Lord cardinal will your gracePersuade the queen to send the Duke of YorkUnto his princely brother presently? she’y, Lord Hastings, go with him, from her jealous arms pluckforce­\n",
      "original text2:  USADA chief says donation offer was a conflict of interest\n",
      "transfer text2:  ... says.­   conflict of interest.. W Englishs\n",
      "--------------------------------------------------\n",
      "original text1:  Ryan Guest, 24, fell 300ft to his death\n",
      "transfer text1:  . 22.f. to his..hsss\n",
      "original text2:  CORIOLANUS:I would they were barbarians--as they are,Though in Rome litter'd--not Romans--as they are not,Though calved i' the porch o' the Capitol--MENENIUS:Be gone;Put not your worthy rage into your tongue;One time will owe another\n",
      "transfer text2:  C -US: they barbarians--as they are,Though in Rome'd--not Romans--as they are not,Though calved i' the porch o' the Capitol--MENENEN:Be gone;Put not your worthy rage into your tongue;One time will ow another were\n",
      "--------------------------------------------------\n",
      "original text1:  DUKE VINCENTIO:Slandering a prince deserves it\n",
      "transfer text1:  INCOPIOing a it! sshstsse\n",
      "original text2:  165,600 more international migrants arrived than emigrants left\n",
      "transfer text2:  3,,  arrived.ts ssssss\n",
      "--------------------------------------------------\n",
      "original text1:  If it is popular it could be rolled out to 37 pubs in the area\n",
      "transfer text1:  you' it it could be to out to 37 pubs in the areasssen ssscoming it\n",
      "original text2:  ROMEO:Go to; I say you shall\n",
      "transfer text2:  to...’ you’’WGo\n",
      "--------------------------------------------------\n",
      "original text1:  Beat the previous warmest month on record of July 1936\n",
      "transfer text1:  . on..sss Comingscoming\n",
      "original text2:  ANTONIO:Which, of he or Adrian, for a goodwager, first begins to crow?SEBASTIAN:The old cock\n",
      "transfer text2:  ON:WhICH, ofhe or Adrian, first a goodwagon, first to crow?SEBASTIAN:: oldcock\n",
      "--------------------------------------------------\n",
      "original text1:  For instance, sir,That you may know you shall not want, one word\n",
      "transfer text1:  , may may you’ not, one word English Englishs,\n",
      "original text2:  Mr Wilson, 65, has suggested he might stand for Kent police commissioner\n",
      "transfer text2:  , has  ’’ stand for.\n",
      "--------------------------------------------------\n",
      "original text1:  Filmmaker Spike Lee riled about New York gentrification and hipsters\n",
      "transfer text1:  Sp Spike  d about New York gentr and hipstersssss s\n",
      "original text2:  YORK:Then leave me not, my lords; be resolute;I mean to take possession of my right\n",
      "transfer text2:  n me?, —ds;be re!ute;I mean to take possession of my\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for index in range(20):\n",
    "    \n",
    "    text_pair = test_df.iloc[index]\n",
    "    text1 = text_pair['sentence']\n",
    "    text2 = text_pair['target_text']\n",
    "\n",
    "    text_id1 = tokenizer(text1, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')['input_ids'].to(device)\n",
    "    text_id2 = tokenizer(text2, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')['input_ids'].to(device)\n",
    "\n",
    "    encoder_outputs1 = model.encoder(input_ids=text_id1)\n",
    "    latent_vector1 = encoder_outputs1.last_hidden_state\n",
    "\n",
    "    encoder_outputs2 = model.encoder(input_ids=text_id2)\n",
    "    latent_vector2 = encoder_outputs2.last_hidden_state\n",
    "\n",
    "    # modify latent vector\n",
    "    latent_vector1_content = latent_vector1[:, :, :384]\n",
    "    latent_vector1_style = latent_vector1[:, :, 384:]\n",
    "\n",
    "    latent_vector2_content = latent_vector2[:, :, :384]\n",
    "    latent_vector2_style = latent_vector2[:, :, 384:]\n",
    "\n",
    "    # swap style\n",
    "    modify_latent_vector1 = torch.cat([latent_vector1_content, latent_vector2_style], dim=-1)\n",
    "    modify_latent_vector2 = torch.cat([latent_vector2_content, latent_vector1_style], dim=-1)\n",
    "\n",
    "    outputs1 = model(decoder_input_ids = text_id1, encoder_outputs=encoder_outputs1)\n",
    "    outputs2 = model(decoder_input_ids = text_id2, encoder_outputs=encoder_outputs2)\n",
    "\n",
    "    logits1 = outputs1.logits\n",
    "    predicted_token_ids1 = torch.argmax(logits1, dim=-1).to(device).flatten()\n",
    "\n",
    "    logits2 = outputs2.logits\n",
    "    predicted_token_ids2 = torch.argmax(logits2, dim=-1).to(device).flatten()\n",
    "\n",
    "    new_text1 = tokenizer.decode(predicted_token_ids1, skip_special_tokens=True)\n",
    "    new_text2 = tokenizer.decode(predicted_token_ids2, skip_special_tokens=True)\n",
    "\n",
    "    print(\"original text1: \", text1)\n",
    "    print(\"transfer text1: \", new_text1)\n",
    "    print(\"original text2: \", text2)\n",
    "    print(\"transfer text2: \", new_text2)\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save_pretrained('./T5_model_sliding')\n",
    "#torch.save(model.state_dict(), './model_save/T5_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ model\n",
    "# model = T5ForConditionalGeneration.from_pretrained('./model_save')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "505",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
