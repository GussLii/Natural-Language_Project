{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch.nn as nn\n",
    "from src.load_dataloader import initial_dataloader\n",
    "from src.load_dataloader import initial_dataloader_cv\n",
    "from src.evaluation import evaluate\n",
    "from src.evaluation import evaluate_process\n",
    "from src.load_config import load_config\n",
    "from src.evaluation import show_sentence\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "model_name = 't5-small'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, max_length = max_length)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_preprocess/datasets_combine.csv')\n",
    "\n",
    "\n",
    "Shakespeare_data=data[data['label']==1]\n",
    "CNN_data=data[data['label']==0]\n",
    "Shakespeare_data_normal,Shakespeare_data_test=train_test_split(Shakespeare_data,test_size=0.009,random_state=42)\n",
    "CNN_data_normal,CNN_data_test=train_test_split(CNN_data,test_size=0.009,random_state=42)\n",
    "data_final_test=pd.concat([Shakespeare_data_test,CNN_data_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shakespeare_data_model1,Shakespeare_data_model2=train_test_split(Shakespeare_data_normal,test_size=0.5,random_state=42)\n",
    "CNN_data_model1,CNN_data_model2=train_test_split(CNN_data_normal,test_size=0.5,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class T5Dataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(cnn_sentences, shakespeare_sentences, tokenizer, max_length=512):\n",
    "    model_inputs = tokenizer(cnn_sentences, max_length=max_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "    # Prepare the labels for the T5 model which should be the IDs of the Shakespeare sentences\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(shakespeare_sentences, max_length=max_length, truncation=True, padding='max_length', return_tensors=\"pt\").input_ids\n",
    "    # Replace padding token id's of the labels by -100 so it's ignored by the loss function\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return T5Dataset({\"input_ids\": model_inputs.input_ids, \"attention_mask\": model_inputs.attention_mask, \"labels\": labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_data_train, CNN_data_val = train_test_split(CNN_data_model1, test_size=0.1, random_state=42)\n",
    "Shakespeare_data_train, Shakespeare_data_val = train_test_split(Shakespeare_data_model1, test_size=0.1, random_state=42)\n",
    "model11_train_dataset = prepare_data(CNN_data_train['sentence'].tolist(), Shakespeare_data_train['sentence'].tolist(), tokenizer)\n",
    "model11_val_dataset = prepare_data(CNN_data_val['sentence'].tolist(), Shakespeare_data_val['sentence'].tolist(), tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_data_train2, CNN_data_val2 = train_test_split(CNN_data_model2, test_size=0.1, random_state=42)\n",
    "Shakespeare_data_train2, Shakespeare_data_val2 = train_test_split(Shakespeare_data_model2, test_size=0.1, random_state=42)\n",
    "model22_train_dataset = prepare_data(CNN_data_train2['sentence'].tolist(), Shakespeare_data_train2['sentence'].tolist(), tokenizer)\n",
    "model22_val_dataset = prepare_data(CNN_data_val2['sentence'].tolist(), Shakespeare_data_val2['sentence'].tolist(), tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model11 = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory for model checkpoints\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size for training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=200,\n",
    "    do_train=True,                   # whether to run training\n",
    "    do_eval=True,                    # whether to run eval on the dev set\n",
    "    evaluation_strategy=\"steps\",     # evaluation strategy to adopt during training\n",
    "    eval_steps=500,                  # evaluation step\n",
    "    save_steps=500,                  # save checkpoint every 500 steps\n",
    "    save_total_limit=1,              # only keep the most recent checkpoint\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model11,\n",
    "    args=training_args,\n",
    "    train_dataset=model11_train_dataset,  # your training dataset\n",
    "    eval_dataset=model11_val_dataset,   # your evaluation dataset\n",
    "    # You can also include a compute_metrics function to calculate metrics during evaluation\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('./saved_model')\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model22 = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args2 = TrainingArguments(\n",
    "    output_dir='./results',          # output directory for model checkpoints\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size for training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=200,\n",
    "    do_train=True,                   # whether to run training\n",
    "    do_eval=True,                    # whether to run eval on the dev set\n",
    "    evaluation_strategy=\"steps\",     # evaluation strategy to adopt during training\n",
    "    eval_steps=500,                  # evaluation step\n",
    "    save_steps=500,                  # save checkpoint every 500 steps\n",
    "    save_total_limit=1,              # only keep the most recent checkpoint\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer2 = Trainer(\n",
    "    model=model11,\n",
    "    args=training_args2,\n",
    "    train_dataset=model22_train_dataset,  # your training dataset\n",
    "    eval_dataset=model22_val_dataset,   # your evaluation dataset\n",
    "    # You can also include a compute_metrics function to calculate metrics during evaluation\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer2.train()\n",
    "\n",
    "# Save the model\n",
    "trainer2.save_model('./saved_model')\n",
    "\n",
    "# Evaluate the model\n",
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(sentence, label, model1, model2, tokenizer, device, max_length=512):\n",
    "    # Choose the model based on the label\n",
    "    model = model2 if label == 1 else model1\n",
    "    \n",
    "    # Tokenize the input sentence\n",
    "    inputs = tokenizer.encode(sentence, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "    \n",
    "    # Generate the output using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_length=max_length)\n",
    "    \n",
    "    # Decode the generated sentence\n",
    "    generated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_sentence\n",
    "\n",
    "# Example usage for generating text with the models\n",
    "def generate_from_test_set(test_set, model1, model2, tokenizer, device):\n",
    "    generated_sentences = []\n",
    "    for index, row in test_set.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        label = row['label']\n",
    "        generated = generate_sentence(sentence, label, model1, model2, tokenizer, device)\n",
    "        generated_sentences.append(generated)\n",
    "        if len(generated_sentences) >= 10:  # Stop after 10 sentences\n",
    "            break\n",
    "    return generated_sentences\n",
    "\n",
    "# Assuming data_final_test is a pandas DataFrame with the test set\n",
    "generated_sentences = generate_from_test_set(data_final_test, model11, model22, tokenizer, device)\n",
    "\n",
    "for original, generated in zip(data_final_test['sentence'].tolist()[:10], generated_sentences):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Generated: {generated}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
