{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch.nn as nn\n",
    "from src.load_dataloader import initial_dataloader_vector_slicing\n",
    "from src.evaluation import initial_LSTM\n",
    "from src.load_config import load_config\n",
    "from src.evaluation import show_sentence\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 8\n",
    "max_length = 128\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, device = load_config(max_length)\n",
    "# import data\n",
    "df = pd.read_csv('data_preprocess/datasets_combine.csv')\n",
    "# df = df[:1000]\n",
    "train_df, val_df,test_df, train_dataset,val_dataset, test_dataset,train_loader, val_loader, test_loader = initial_dataloader_vector_slicing(df, tokenizer, max_length,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = initial_LSTM(tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(texts1, transfer_labels, lstm_model):\n",
    "    # Assuming transfer_labels is a tensor of shape [batch_size]\n",
    "    # and predicted_token_ids is of shape [batch_size, seq_len]\n",
    "    predicted_token_ids = tokenizer(texts1, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')['input_ids'].to(device)\n",
    "    \n",
    "    # Initialize CrossEntropyLoss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize a tensor to store LSTM outputs\n",
    "    lstm_outputs = torch.empty(size=(predicted_token_ids.shape[0], 2)).to(device)  # 2 for binary classification\n",
    "\n",
    "    # Process each item in the batch\n",
    "    for idx, token_ids in enumerate(predicted_token_ids):\n",
    "        lstm_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Assuming lstm_model.predict returns a tensor of shape [1, 2] (probability for each class)\n",
    "            lstm_output = lstm_model(token_ids.flatten())  # Modify this call according to your LSTM model's interface\n",
    "            lstm_outputs[idx] = lstm_output\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(lstm_outputs, transfer_labels)\n",
    "\n",
    "    return loss, lstm_outputs, transfer_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]c:\\Users\\Ene\\Desktop\\CS505_github\\Natural-Language_Project\\src\\load_dataloader.py:179: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids1': torch.tensor(item['input_ids1'], dtype=torch.long).squeeze(0),\n",
      "c:\\Users\\Ene\\Desktop\\CS505_github\\Natural-Language_Project\\src\\load_dataloader.py:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids2': torch.tensor(item['input_ids2'], dtype=torch.long).squeeze(0),\n",
      "c:\\Users\\Ene\\Desktop\\CS505_github\\Natural-Language_Project\\src\\load_dataloader.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask1': torch.tensor(item['attention_mask1'], dtype=torch.long).squeeze(0),\n",
      "c:\\Users\\Ene\\Desktop\\CS505_github\\Natural-Language_Project\\src\\load_dataloader.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask2': torch.tensor(item['attention_mask2'], dtype=torch.long).squeeze(0),\n",
      "c:\\Users\\Ene\\Desktop\\CS505_github\\Natural-Language_Project\\src\\load_dataloader.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels1': torch.tensor(item['labels1'], dtype=torch.long).squeeze(0),\n",
      "c:\\Users\\Ene\\Desktop\\CS505_github\\Natural-Language_Project\\src\\load_dataloader.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels2': torch.tensor(item['labels2'], dtype=torch.long).squeeze(0),\n",
      " 10%|█         | 1/10 [1:08:42<10:18:26, 4122.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [1:32:48<5:39:44, 2548.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [1:53:01<3:46:07, 1938.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [2:13:16<2:45:17, 1652.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [2:26:53<1:52:36, 1351.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [2:40:27<1:17:54, 1168.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [2:53:57<52:34, 1051.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [3:07:27<32:28, 974.48s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [3:20:55<15:22, 922.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [3:34:25<00:00, 1286.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 10\n",
    "model.to(device)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids1 = batch['input_ids1'].to(device)\n",
    "        attention_mask1 = batch['attention_mask1'].to(device)\n",
    "        labels1 = batch['labels1'].to(device)\n",
    "        \n",
    "        input_ids2 = batch['input_ids2'].to(device)\n",
    "        attention_mask2 = batch['attention_mask2'].to(device)\n",
    "        labels2 = batch['labels2'].to(device)\n",
    "        \n",
    "        # first reconstruct the two sentences\n",
    "        \n",
    "        outputs1 = model(input_ids=input_ids1, attention_mask=attention_mask1, labels=labels1)\n",
    "        outputs2 = model(input_ids=input_ids2, attention_mask=attention_mask2, labels=labels2)\n",
    "        \n",
    "        loss = outputs1.loss + outputs2.loss\n",
    "        \n",
    "        # ----------------------- perform style transfer -----------------------\n",
    "        # call model encoder\n",
    "        encoder_outputs1 = model.encoder(input_ids=input_ids1)\n",
    "        latent_vector1 = encoder_outputs1.last_hidden_state\n",
    "        \n",
    "        encoder_outputs2 = model.encoder(input_ids=input_ids2)\n",
    "        latent_vector2 = encoder_outputs2.last_hidden_state\n",
    "\n",
    "        # modify latent vector\n",
    "        latent_vector1_content = latent_vector1[:, :, :384]\n",
    "        latent_vector1_style = latent_vector1[:, :, 384:]\n",
    "        \n",
    "        latent_vector2_content = latent_vector2[:, :, :384]\n",
    "        latent_vector2_style = latent_vector2[:, :, 384:]\n",
    "        \n",
    "        # swap style\n",
    "        modify_latent_vector1 = torch.cat([latent_vector1_content, latent_vector2_style], dim=-1)\n",
    "        modify_latent_vector2 = torch.cat([latent_vector2_content, latent_vector1_style], dim=-1)\n",
    "        \n",
    "        encoder_outputs1.last_hidden_state = modify_latent_vector1\n",
    "        encoder_outputs2.last_hidden_state = modify_latent_vector2\n",
    "        \n",
    "        decoder_input_text = \"style transfer:\"\n",
    "        decoder_input_id = tokenizer(decoder_input_text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')['input_ids'].to(device)\n",
    "        # sahpe of decoder_input_id should be [batch_size, seq_max_len]\n",
    "        decoder_input_ids = decoder_input_id.repeat(input_ids1.shape[0], 1)\n",
    "        \n",
    "        outputs1 = model(decoder_input_ids = decoder_input_ids, encoder_outputs=encoder_outputs1)\n",
    "        outputs2 = model(decoder_input_ids = decoder_input_ids, encoder_outputs=encoder_outputs2)\n",
    "\n",
    "        logits1 = outputs1.logits\n",
    "        predicted_token_ids1 = torch.argmax(logits1, dim=-1).to(device)\n",
    "        transfer_labels1 = batch['sentence2_style'].to(device)\n",
    "        \n",
    "        logits2 = outputs2.logits\n",
    "        predicted_token_ids2 = torch.argmax(logits2, dim=-1).to(device)\n",
    "        transfer_labels2 = batch['sentence1_style'].to(device)\n",
    "        \n",
    "        # sent to LSTM model\n",
    "\n",
    "        texts1 = tokenizer.batch_decode(predicted_token_ids1, skip_special_tokens=True)\n",
    "        loss1,_,_ = custom_loss_function(texts1, transfer_labels1, LSTM_model)\n",
    "            \n",
    "        texts2 = tokenizer.batch_decode(predicted_token_ids2, skip_special_tokens=True)\n",
    "        loss2,_,_ = custom_loss_function(texts2, transfer_labels2, LSTM_model)  \n",
    "\n",
    "        # Accumulate the losses\n",
    "        #print(\"-----------------------\")\n",
    "        #print(loss.item())\n",
    "        loss += loss1.detach() + loss2.detach()\n",
    "        #print(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch in val_loader:\n",
    "        input_ids1 = batch['input_ids1'].to(device)\n",
    "        attention_mask1 = batch['attention_mask1'].to(device)\n",
    "        labels1 = batch['labels1'].to(device)\n",
    "        \n",
    "        input_ids2 = batch['input_ids2'].to(device)\n",
    "        attention_mask2 = batch['attention_mask2'].to(device)\n",
    "        labels2 = batch['labels2'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs1 = model(input_ids=input_ids1, attention_mask=attention_mask1, labels=labels1)\n",
    "            outputs2 = model(input_ids=input_ids2, attention_mask=attention_mask2, labels=labels2)\n",
    "            \n",
    "            loss = outputs1.loss + outputs2.loss\n",
    "            \n",
    "            logits1 = outputs1.logits\n",
    "            logits2 = outputs2.logits\n",
    "            \n",
    "            predicted_token_ids1 = torch.argmax(logits1, dim=-1).to(device)\n",
    "            predicted_token_ids2 = torch.argmax(logits2, dim=-1).to(device)\n",
    "            \n",
    "            transfer_labels1 = batch['sentence2_style'].to(device)\n",
    "            transfer_labels2 = batch['sentence1_style'].to(device)\n",
    "            \n",
    "            texts1 = tokenizer.batch_decode(predicted_token_ids1, skip_special_tokens=True)\n",
    "            texts2 = tokenizer.batch_decode(predicted_token_ids2, skip_special_tokens=True)\n",
    "            \n",
    "            loss1, lstm_outputs1, transfer_labels1 = custom_loss_function(texts1, transfer_labels1, LSTM_model)\n",
    "            loss2, lstm_outputs2, transfer_labels2 = custom_loss_function(texts2, transfer_labels2, LSTM_model)\n",
    "            \n",
    "            total_lstm_loss = loss1 + loss2\n",
    "            \n",
    "            loss += total_lstm_loss\n",
    "            \n",
    "            total_eval_loss += loss.item()\n",
    "            \n",
    "            # get accuracy by using argmax \n",
    "            predicted_labels1 = torch.argmax(lstm_outputs1, dim=-1)\n",
    "            predicted_labels2 = torch.argmax(lstm_outputs2, dim=-1)\n",
    "            \n",
    "            total_eval_accuracy += (predicted_labels1 == transfer_labels1).sum().item()\n",
    "            total_eval_accuracy += (predicted_labels2 == transfer_labels2).sum().item()\n",
    "            \n",
    "            nb_eval_steps += 1\n",
    "    print(\"Validation Accuracy: {0:.4f}\".format(total_eval_accuracy/nb_eval_steps))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text1:  You are acounsellor; if you can command these elements tosilence, and work the peace of the present, we willnot hand a rope more; use your authority: if youcannot, give thanks you have lived so long, and makeyourself ready in your cabin for the mischance ofthe hour, if it so hap\n",
      "transfer text1:  –s,, if you can command these elements tosilence, and work the peace of the present, we willnot hand a rope more; use your authority: if youcannot, give thanks you have lived so long, and makeyourself ready in your cabin for the mischance ofthe hour, if it so hap\n",
      "original text2:  Actor's wife, Emma Heming-Willis, cast doubt on the report Monday on Twitter\n",
      "transfer text2:  ,, Emma Heming-Willis, cast doubt on the report Monday on Twitter\n",
      "--------------------------------------------------\n",
      "original text1:  Morgan Schneiderlin put in the hard yards for Southampton at Anfield\n",
      "transfer text1:  put putin put in the hard yards for Southampton Anfield\n",
      "original text2:  KING EDWARD IV:Leave me, or tarry, Edward will be king,And not be tied unto his brother's will\n",
      "transfer text2:  DWARD IV:Leave me, or tarry, Edward will be king,And not be tied unto his brother's will\n",
      "--------------------------------------------------\n",
      "original text1:  MARIANA:You have not been inquired after:I have sat here all day\n",
      "transfer text1:  :You have not been inquired after:I have sat here all day\n",
      "original text2:  More than 20 per cent of women said that chocolate would be the last thing they could give up, according to aa Cancer Research UK campaign\n",
      "transfer text2:  cent of women said that chocolate would be the last thing they could give up, according to aa Cancer Research UK campaign\n",
      "--------------------------------------------------\n",
      "original text1:  Kaka currently in Brazil with the national team ahead of a World Cup qualifier\n",
      "transfer text1:  K currently in Brazil with the national team ahead of a World Cup qualifier\n",
      "original text2:  Come, bring away thy pack afterme\n",
      "transfer text2:  thy packme\n",
      "--------------------------------------------------\n",
      "original text1:  Widow:She shall not\n",
      "transfer text1:  Wi not\n",
      "original text2:  Former Virginia Gov Bob McDonnell is currently facing charge he performed 'official acts' to benefit Star Scientific CEO Jonnie Williams\n",
      "transfer text2:  Donnell is currently facing charge he performed 'official acts' to benefit Star Scientific CEOnnie Williams\n",
      "--------------------------------------------------\n",
      "original text1:  MENENIUS:Put them not to't:Pray you, go fit you to the custom andTake to you, as your predecessors have,Your honour with your form\n",
      "transfer text1:  US:Put them not to't:Pray you, as fit you to the custom andTake to you, as your predecessors have,Your honour with your\n",
      "original text2:  Maria Sharapova will play Li Na in final of WTA event in Stuttgart\n",
      "transfer text2:  Mariaaova will play Li Na in final of WTA event in Stuttgart\n",
      "--------------------------------------------------\n",
      "original text1:  Discrepancy in female promotions may have been magnified by recession\n",
      "transfer text1:  pancy in female promotions may have been magnified by recession\n",
      "original text2:  KING RICHARD III:What is his name?Page:His name, my lord, is Tyrrel\n",
      "transfer text2:  ICHARD III:What is his name?Page:His, my lord, is Tyrrel\n",
      "--------------------------------------------------\n",
      "original text1:  The 55-year-old has battled her grim diagnosis with bravery and humour\n",
      "transfer text1:  old-old has battled her grim diagnosis with bravery and humour\n",
      "original text2:  Mariner:Make your best haste, and go notToo far i' the land: 'tis like to be loud weather;Besides, this place is famous for the creaturesOf prey that keep upon't\n",
      "transfer text2:  your best haste, and go notToo far i' the land: 'tis like to be loud weather;Besides, this place is famous for the creaturesOf prey that keep on't\n",
      "--------------------------------------------------\n",
      "original text1:  Wayne Ingold attacked with chemical liquid outside Essex flats\n",
      "transfer text1:  attacked with chemical outside Essex flats\n",
      "original text2:  Seventh Citizen:Therefore let him be consul: the gods give him joy,and make him good friend to the people!All Citizens:Amen, amen\n",
      "transfer text2:  :zen:Thefore let him be consul: the gods give him joy,and make him good friend to the people!All Citizens:Amen, amen\n",
      "--------------------------------------------------\n",
      "original text1:  Inquest told she had earphones in when she was knocked down\n",
      "transfer text1:  told she had earphones in when she was knocked down\n",
      "original text2:  First Lord:So please you, sir, their speedHath been beyond account\n",
      "transfer text2:  please you, sir, their speedHath been beyond account\n",
      "--------------------------------------------------\n",
      "original text1:  The semiretired psychiatric nurse kept busy looking after people in her job and in her family, and paid limited attention to her own health\n",
      "transfer text1:  red psychiatric nurse kept busy looking after people in her job and in her family, and paid limited attention to her own health\n",
      "original text2:  Tend to themaster's whistle\n",
      "transfer text2:  's\n",
      "--------------------------------------------------\n",
      "original text1:  Bush also suggested Obama is losing the war against the Islamic State\n",
      "transfer text1:  Bush suggested Obama is losing the war against the Islamic State\n",
      "original text2:  COMINIUS:Nay, come away\n",
      "transfer text2:  COIUS: comeay, come away\n",
      "--------------------------------------------------\n",
      "original text1:  The honour'd godsKeep Rome in safety, and the chairs of justiceSupplied with worthy men! plant love among 's!Throng our large temples with the shows of peace,And not our streets with war!First Senator:Amen, amen\n",
      "transfer text1:  dsKeep Rome in safety, and the chairs of justiceSupplied with worthy men! love among's!Throng our large temples with the shows of peace,And not our streets with war!First Senator:Amen, amen\n",
      "original text2:  Models Miranda Kerr and Rosie Huntington-Whitely swear by superfoods\n",
      "transfer text2:  r and Rosie Huntington-Whitely swear by superfoods\n",
      "--------------------------------------------------\n",
      "original text1:  She veered off the road and down the verge before crashing into a tree\n",
      "transfer text1:  a off the ver and down the verge crashing into a tree\n",
      "original text2:  And is Aufidius with him? You are theyThat made the air unwholesome, when you castYour stinking greasy caps in hooting atCoriolanus' exile\n",
      "transfer text2:  ?idius with him? areThat made the air unwholesome, when you castYour stinking greasy caps in hooting atCoriolanus'\n",
      "--------------------------------------------------\n",
      "original text1:  QUEEN:Why hopest thou so? 'tis better hope he is;For his designs crave haste, his haste good hope:Then wherefore dost thou hope he is not shipp'd?GREEN:That he, our hope, might have retired his power,And driven into despair an enemy's hope,Who strongly hath set footing in this land:The banish'd Bolingbroke repeals himself,And with uplifted arms is safe arrivedAt Ravenspurgh\n",
      "transfer text1:  hopest thou so??tis better hope he is;For his designs crave haste, his haste good hope:Then wherefore dost thou hope he is not shipp'd?GREEN:That he, our hope, might have retired his power,And driven into despair an enemy's hope,Who strongly hath set footing in this land:The banish'd Bolingbroke repeals himself,And with uplifted ::ts,gh\n",
      "original text2:  He told his young wives they were 'honourable vessels' and journalised about how they were 'willing to obey'\n",
      "transfer text2:  his  they were 'willonourable vessels' and journalised about how they were 'willing to obey'\n",
      "--------------------------------------------------\n",
      "original text1:  Stay was denied, Cecil Johnson Jr\n",
      "transfer text1:  Cecil Johnson Jr\n",
      "original text2:  BAPTISTA:I am glad he's come, howsoe'er he comes\n",
      "transfer text2:  A:I am glad he's come, howsoe'er he comes\n",
      "--------------------------------------------------\n",
      "original text1:  Lando Hite hid in stall which collapsed on him as it was battered by winds\n",
      "transfer text1:  e as... in... which collapsed on him as it was battered by winds\n",
      "original text2:  He sits in his state, as a thing made forAlexander\n",
      "transfer text2:  s his state, as a thing made forAlexander\n",
      "--------------------------------------------------\n",
      "original text1:  KING RICHARD III:Why then, by God--QUEEN ELIZABETH:God's wrong is most of all\n",
      "transfer text1:  ICHARD III:Why then, by God--QUEEN ELIZABETH:God's wrong is most of all\n",
      "original text2:  Terry and friends Tudor Musteata and Stephen Niland had been drinking in a pub near Fenchurch Street Station before confrontation, court told\n",
      "transfer text2:  Stephen Tudoreata and Stephen Niland had been drinking in a pub near Fenchurch Street Station before confrontation, court told\n",
      "--------------------------------------------------\n",
      "original text1:  The 49-year-old has five daughters who he calls 'Hitler's children'He earns a living as a Hitler look-a-like and charges tourists for pictures\n",
      "transfer text1:  -year-old has five daughters who he calls 'Hitler's children'He earns a living as a Hitler look-a-like and charges tourists for pictures\n",
      "original text2:  But come, my lord; and with a heavy heart,Thinking on them, go I unto the Tower\n",
      "transfer text2:  ..., my...d, and with a heavy heart,Thinking on them, go I unto the Tower\n",
      "--------------------------------------------------\n",
      "original text1:  You say that Edward is your brother's son:So say we too, but not by Edward's wife;For first he was contract to Lady Lucy--Your mother lives a witness to that vow--And afterward by substitute betroth'dTo Bona, sister to the King of France\n",
      "transfer text1:  you is your's son:For say we too, but not by Edward's wife;For first he was contract to Lady Lucy--Your mother lives a witness to that vow--And afterward by substitute betroth'dTo Bona, sister to the King of France\n",
      "original text2:  Bo Xilai to face trial Thursday on charges of corruption, bribery and abuse of power\n",
      "transfer text2:  –ai to face trial Thursday on charges of corruption, bribery and abuse of power\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for index in range(20):\n",
    "    \n",
    "    text_pair = test_df.iloc[index]\n",
    "    text1 = text_pair['sentence']\n",
    "    text2 = text_pair['target_text']\n",
    "\n",
    "    text_id1 = tokenizer(text1, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')['input_ids'].to(device)\n",
    "    text_id2 = tokenizer(text2, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')['input_ids'].to(device)\n",
    "\n",
    "    encoder_outputs1 = model.encoder(input_ids=text_id1)\n",
    "    latent_vector1 = encoder_outputs1.last_hidden_state\n",
    "\n",
    "    encoder_outputs2 = model.encoder(input_ids=text_id2)\n",
    "    latent_vector2 = encoder_outputs2.last_hidden_state\n",
    "\n",
    "    # modify latent vector\n",
    "    latent_vector1_content = latent_vector1[:, :, :384]\n",
    "    latent_vector1_style = latent_vector1[:, :, 384:]\n",
    "\n",
    "    latent_vector2_content = latent_vector2[:, :, :384]\n",
    "    latent_vector2_style = latent_vector2[:, :, 384:]\n",
    "\n",
    "    # swap style\n",
    "    modify_latent_vector1 = torch.cat([latent_vector1_content, latent_vector2_style], dim=-1)\n",
    "    modify_latent_vector2 = torch.cat([latent_vector2_content, latent_vector1_style], dim=-1)\n",
    "\n",
    "    outputs1 = model(decoder_input_ids = text_id1, encoder_outputs=encoder_outputs1)\n",
    "    outputs2 = model(decoder_input_ids = text_id2, encoder_outputs=encoder_outputs2)\n",
    "\n",
    "    logits1 = outputs1.logits\n",
    "    predicted_token_ids1 = torch.argmax(logits1, dim=-1).to(device).flatten()\n",
    "\n",
    "    logits2 = outputs2.logits\n",
    "    predicted_token_ids2 = torch.argmax(logits2, dim=-1).to(device).flatten()\n",
    "\n",
    "    new_text1 = tokenizer.decode(predicted_token_ids1, skip_special_tokens=True)\n",
    "    new_text2 = tokenizer.decode(predicted_token_ids2, skip_special_tokens=True)\n",
    "\n",
    "    print(\"original text1: \", text1)\n",
    "    print(\"transfer text1: \", new_text1)\n",
    "    print(\"original text2: \", text2)\n",
    "    print(\"transfer text2: \", new_text2)\n",
    "    print(\"--------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "505",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
